% MLSys 2025 paper skeleton (reset)
\documentclass{article}

% Recommended packages
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}

% MLSys style
\usepackage{mlsys2025}
% \usepackage[accepted]{mlsys2025}

% Running title (short)
\mlsystitlerunning{Theoretical Grounds for Popularity Bias in Two-Tower Models}

\begin{document}

\twocolumn[
\mlsystitle{Theoretical Grounds for Popularity Bias in Two-Tower Models Trained with Cosine-Based Loss}

% Author and affiliation block (kept in source; hidden in blind review)
\mlsyssetsymbol{equal}{*}
\begin{mlsysauthorlist}
\mlsysauthor{Andrey Atamanyuk}{wb}
\end{mlsysauthorlist}

\mlsysaffiliation{wb}{Wildberries and Russ, Russia}
\mlsyscorrespondingauthor{Andrey Atamanyuk}{atamanyuk.andrey@rwb.ru}

% Keywords are optional
\mlsyskeywords{Representation learning, Two-tower models, Embedding norms}

\vskip 0.3in
\begin{abstract}
    Two-tower models may exhibit popularity bias, observed as a positive correlation between item frequency and its embedding norm that inflates dot-product scores at ranking time. This phenomenon is due to specific properties of loss and encoder architecture. Using analysis of encoder parameters’ updates, I identify sufficient conditions under which embedding updates are provably orthogonal to the current embedding and hence monotonically increase its norm. My theory yields explicit guarantees for the emergence of popularity bias in practical two-tower setups (InfoNCE/cosine-based loss, asymmetric two-tower architecture) and corrects a common misconception in prior works that orthogonality of the gradient alone implies norm inflation for deep encoders.  Empirical studies support the theory via geometry-first investigation: configurations that satisfy these premises exhibit strictly orthogonal embedding movements and a robust statistically significant frequency–norm coupling, whereas violations of any premise break orthogonality and yield non-systematic, noisy update trajectories. The results provide theoretical grounds for popularity bias in cosine-trained two-tower models and show when it should be expected in production systems.
\end{abstract}
]

% Footnote line (hidden unless [accepted])
\printAffiliationsAndNotice{\mlsysEqualContribution}

\input{sections/introduction}
\input{sections/sufficient_conditions}
\input{sections/norm_growth_vs_popularity}
\input{sections/experiments}

% References
\bibliography{references}
\bibliographystyle{mlsys2025}

\appendix
\input{sections/appendix_encoders}
\input{sections/appendix_cosine_lemma}
\input{sections/appendix_cosine_gradient_magnitude}
\input{sections/appendix_coupling_popularity}
\input{sections/appendix_note_on_distributions}
\input{sections/appendix_note_on_batch_difference}
\input{sections/appendix_note_on_c_variability}

\end{document}
