\section{Encoder Examples: Parameter-Linear and Nonlinear}\label{app:encoders}

\subsection{Encoder with a Single Embedding Layer}\label{app:embed}
\textbf{Input:} $x_i = i \in \{1,\dots,N\}$ \;($x_i$ -- index of example $i$).

\textbf{Parameters:} $E\in\mathbb{R}^{N\times d}$, \;$\theta=E$, \;$\theta_{i,n}=E_{i,n}$ \;(here $n$ is the column index).

\textbf{Output:} $q(\theta,x_i)=E_i\in\mathbb{R}^{d}$, \;$q_i = q(\theta,x_i)$.

Let $m$ denote an output coordinate of $q$, and let $i'$ and $n'$ be arbitrary row/column indices with respect to which the derivative is taken (they range over all parameters). I also vectorize $\theta$ from an $N\times d$ matrix into a vector of length $N\!\cdot\! d$.

\textbf{Jacobian:}
\begin{align}
J_i \,=\, \frac{\partial q_i}{\partial \theta} \,=\, \begin{cases}
1, & \text{if } i' = i,\; n' = m,\\
0, & \text{otherwise.}
\end{cases}
\end{align}

Here $J_i$ is a $d \times (N\!\cdot\! d)$ table that shows how each output coordinate changes under an infinitesimal change of each parameter at fixed input $x_i$.

Equivalently, all entries are zero except the $d\times d$ block corresponding to row $i$, which is the identity $I_d$:
\begin{align}
J_i \,=\, \Bigl[\, \underbrace{0_{d\times (i-1)d}}_{\text{params }1\ldots(i-1)} \;\Big|\; \underbrace{I_d}_{\text{params of row }i} \;\Big|\; \underbrace{0_{d\times (N-i)d}}_{\text{params }(i+1)\ldots N} \Bigr].
\end{align}
Thus $J$ does not depend on $\theta$, and the encoder is parameter-linear.

\subsection{Encoder with a Single Linear Layer (No Bias)}\label{app:linear}
\textbf{Input:} $x_i = (x_{i,1},\dots,x_{i,h})^{\!\top} \in \mathbb{R}^{h}$ \;($x_i$ -- feature vector of example $i$).

\textbf{Parameters:} $W\in\mathbb{R}^{d\times h}$, \;$\theta=W$, \;$\theta_{m,n}=W_{m,n}$ \;(here $m$ indexes output rows and $n$ indexes input features).

\textbf{Output:} $q(\theta,x_i)=W\,x_i\in\mathbb{R}^{d}$, \;$q_i=q(\theta,x_i)$.

Let $m$ denote an output coordinate of $q_i$, and let $m'$ and $n'$ be arbitrary row/column indices with respect to which the derivative is taken (they range over all parameters). I also vectorize $\theta$ from a $d\!\times\!h$ matrix into a vector of length $d\!\cdot\!h$.

\textbf{Jacobian:}
\begin{align}
J_i \,=\, \frac{\partial q_i}{\partial \theta} \,=\, \begin{cases}
x_{i,n'}, & \text{if } m' = m,\\
0, & \text{otherwise.}
\end{cases}
\end{align}

Here $J_i$ is a $d \times (d\!\cdot\! h)$ table showing how each output coordinate changes under an infinitesimal change of each parameter at fixed input $x_i$. Equivalently, in each of the $d$ blocks (width $h$) on row $m$ there is a copy of $x_i^{\!\top}$, the remaining entries are zero:
\begin{align}
J_i \,=\, \operatorname{diag}\!\bigl(\underbrace{x_i^{\!\top},\, x_i^{\!\top},\, \dots,\, x_i^{\!\top}}_{d\ \text{times}}\bigr) .
\end{align}
Hence $J$ depends on the input $x_i$ but not on $\theta$, so this encoder is parameter-linear. If a bias is present, append an identity block on the right.

\subsection{Encoder with Two Consecutive Linear Layers (No Bias)}\label{app:two-linear}
\textbf{Input:} $x_i = (x_{i,1},\dots,x_{i,h})^{\!\top} \in \mathbb{R}^{h}$ \;($x_i$ -- feature vector of example $i$).

\textbf{Parameters:} $A\in\mathbb{R}^{d_1\times h}$, $B\in\mathbb{R}^{d_2\times d_1}$, \;$\theta=\{A,B\}$, \;$\theta^{(A)}_{r,s}=A_{r,s}$, \;$\theta^{(B)}_{m,n}=B_{m,n}$ \;(here $r$ is a “hidden” coordinate, $m$ an output coordinate).

\textbf{Output:} $z_i = A\,x_i \in \mathbb{R}^{d_1}$, \; $q_i = q(\theta,x_i) = B\,z_i \in \mathbb{R}^{d_2}$.

Let $m$ denote an output coordinate of $q_i$. Let $m',n'$ be arbitrary row/column indices in $B$, and $r',s'$ indices in $A$.

\textbf{W.r.t. $B$:}
\begin{align}
\frac{\partial q_{i,m}}{\partial B_{m',n'}} 
= \begin{cases} z_{i,n'}, & m' = m, \\ 0, & \text{otherwise.} \end{cases}
\end{align}

\textbf{W.r.t. $A$:}
\begin{align}
\frac{\partial q_{i,m}}{\partial A_{r',s'}} = B_{m,r'}\,x_{i,s'} .
\end{align}

Therefore,
\begin{align}
J_i = \bigl[\, J^{(B)}_i \;\big|\; J^{(A)}_i \bigr],
\end{align}
where $J^{(B)}_i\in\mathbb{R}^{d_2\times(d_2 d_1)}$ has, in each of the $d_2$ blocks of width $d_1$, the row $z_i^{\!\top}$, and $J^{(A)}_i\in\mathbb{R}^{d_2\times(d_1 h)}$ consists of $d_1 h$ columns of the form $B_{:,r'}\,x_{i,s'}$.

Consequently, $J_i$ depends on the parameters (through $B$), so this encoder is \emph{not} parameter-linear. Intuitively, although the encoder is linear in the \emph{input}, changing a parameter in the first linear layer affects the output through a multiplication by the second layer, which makes the Jacobian parameter-dependent; hence strict parameter-linearity fails.

