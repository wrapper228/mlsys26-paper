\subsection{Relation to popular papers on orthogonality of embedding motion}

In \emph{NormFace: L2 Hypersphere Embedding for Face Verification} (\textasciitilde930 citations) a new way to train deep encoders is proposed. In Section~3.2 they prove that, at the model output, for $x$ and the gradient $\partial\mathcal{L}/\partial x$, orthogonality holds, after which they immediately make the logical transition \emph{``it can be inferred that after update, $\|x\|^2$ always increases''}, i.e., they implicitly assume that if the loss gradient with respect to the model output is orthogonal to the model output, then this output itself also moves orthogonally. This transition is not obvious: it switches from orthogonality of the gradient to orthogonality of the update itself.

Such orthogonality of the step is guaranteed only under a sufficiently strict set of conditions. I write these conditions explicitly and show that, when they are satisfied, embeddings do move orthogonally; outside this region the transition from orthogonality of the gradient to orthogonal motion of the embedding (and hence to monotone growth of the embedding norm) may fail. Consequently, the motivation for modifying training \emph{because the update increases the point's norm} should be treated as a qualified claim: it is true in the configurations described by my conditions, but it is not a universal basis for changing how encoders are trained—especially deeper ones.

The same critique applies to \emph{The Hidden Pitfalls of the Cosine Similarity Loss} and \emph{On the Importance of Embedding Norms in Self-Supervised Learning}. In particular, the former states that \emph{``The gradient is orthogonal to the embedding, so a step of gradient descent can only increase the embedding's magnitude''} and that \emph{``these derivations are extremely general—they hold across deep learning architectures''}, without accounting for the fact that the gradient is taken with respect to the model output rather than the parameters; it also supports the claim only by observing norm growth in experiments, without analyzing whether that growth is indeed caused by orthogonal motion of the embedding. The latter states that \emph{``Because gradient updates are orthogonal to an embedding point, they must increase the point’s norm''}, which is correct only after refining the conditions under which the \emph{updates} (and not just the gradient) are orthogonal. Such a simplification can mislead the reader about the universality of the "inevitable catch-22".

 It should be acknowledged that in Appendix~B they provide a qualification, stating \emph{``We optimize the cosine similarity by performing standard gradient descent on the embeddings themselves''}, which implies a shallow architecture such as an Embedding Layer; however, this architectural restriction is fixed only in the appendix, while the main text presents the correctness of the claim \emph{``Because gradient updates are orthogonal to an embedding point, they must increase the point’s norm''} without specifying the architecture.

When the conditions above do hold, my analytic proof of orthogonal embedding motion indeed implies growth of the embedding norm (by the Pythagorean theorem). A natural question then arises: how should this growth be evaluated analytically?


