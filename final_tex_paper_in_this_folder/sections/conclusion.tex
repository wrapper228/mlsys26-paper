\section{Conclusion}

If the following factors hold simultaneously for the item tower, then item embeddings of more popular items systematically attain larger norms than those of less popular items:
\begin{enumerate}
  \item optimization uses SGD without momentum and without regularization (the only way to guarantee strict movement defined by equations), 
  \item the encoder is linear in its parameters (parameter-linear),
  \item each distinct input has a dedicated, non-shared parameter row (no overlap across inputs),
  \item the loss gradient with respect to the encoder output (the embedding) is orthogonal to that output,
  \item the learning rate is sufficiently large (orthogonality itself does not require this; the point is to overcome the random initialization of embeddings by ensuring sufficiently large orthogonal displacement).
\end{enumerate}

This list makes explicit how many requirements must be met so that embeddings move orthogonally, their norms grow monotonically, and this growth yields popularity bias. In this way, our investigation refines the takeaways of prior work by specifying exactly which factors guarantee orthogonal embedding movement. To the best of my knowledge, these aspects have not been articulated in the literature on representation learning with cosine–based objectives.

An important observation is that, although these strict conditions restrict the encoder architecture for which we can provide analytic guarantees of popularity bias, modern two–tower systems contain two independent encoders. Even if one of them is a complex deep model with a complicated training dynamics, the other one—provided its architecture is sufficiently primitive—already guarantees the emergence of the phenomenon when trained with a cosine loss. This observation allows us to transfer the present mathematical guarantees to practical systems where, for example, the user tower is a transformer over his history with a sophisticated optimizer, while the item tower is a simple embedding layer trained with SGD without momentum and without regularization.

Thus, we revisited the academic stance on the geometry of embedding movement under cosine–based training. Although orthogonality of movement (and the resulting popularity bias) requires strict constraints on the encoder architecture, popular two–tower designs can still employ a deliberately simple item tower that provably preserves orthogonal movement and leads to popularity bias. 
This may be critical for downstream ranking tasks -- for example, index search based on dot products -- if this property of the training process is not taken into account.
