\subsection{Proposition: When Embeddings Move Orthogonally}

If, in a two-tower model, the encoder satisfies the following four conditions simultaneously:
\begin{enumerate}
  \item optimization uses SGD without momentum and without regularization,
  \item the encoder is linear in its parameters (parameter-linear),
  \item each distinct input has a dedicated, non-shared parameter row (no overlap across inputs),
  \item the loss gradient with respect to the encoder output (the embedding) is orthogonal to that output,
\end{enumerate}

then it can be proved analytically that, during training, the embedding moves orthogonally, i.e., tangentially to the hypersphere on which it located at the previous step.
