\section{Popularity Dependence via Coupling}
\label{app:popularity-dependence}

We aim to show that, after $T$ training batches, the embedding norm of item $i$ is nondecreasing in its sampling probability $p_i$:
\begin{equation}
\mathbb E\,\bigl\|q_i^{(T)}\bigr\| \;\text{ is nondecreasing in }\; p_i.
\end{equation}
We employ the coupling method from probability theory (see "Modern Discrete Probability: An Essential Toolkit").

\paragraph{1) Two runs on the same randomness (we fixed the random outcome of batch generation).}
Run training twice, differing only in the sampling probability of item $i$: $p_i' < p_i''$. At each step $t$ and for every slot of the batch draw the same random number $U \sim \mathrm{Uniform}(0,1)$:
\begin{itemize}
  \item in the run with parameter $p$, place $i$ into the slot if $U \le p_i$;
  \item otherwise place the same non-$i$ item in both runs (chosen using the same random generator).
\end{itemize}
Then, for the batch collected at step $t$,
\begin{equation}
\begin{aligned}
&\mathbf{Indicator}\{\text{item } i \text{ appears at least once under } p_i''\} \\
&\;\ge\; \\
&\mathbf{Indicator}\{\text{item } i \text{ appears at least once under } p_i'\}.
\end{aligned}
\end{equation}
Thus, with the larger $p_i$ the number of appearances of $i$ is never smaller.

Look for X attachment for extra comments on "Note on distributions".

\paragraph{2) What a single occurrence does to the norm.}
Let $s^{(t)} = \bigl\|q_i^{(t)}\bigr\|^2$. One-step update:
\begin{equation}
s^{(t+1)} = \begin{cases}
s^{(t)}, & \text{if $i$ does not occur,} \\
s^{(t)} + \Delta\bigl(s^{(t)}\bigr), & \text{if $i$ does occur.}
\end{cases}
\end{equation}
where $\Delta(s) = \bigl\|\Delta q^{(t)}\bigr\|^2 \ge 0$. For the cosine loss (the gradient is orthogonal to $q$), the dependence is
\begin{equation}
\Delta(s) \,=\, \frac{c}{s}, \qquad \text{see attachment Z}.
\end{equation}
Here $c > 0$: the larger the norm, the smaller the increment, yet the increment remains strictly positive.

\paragraph{3) Requirement: the step must not reverse the order.}
For the final conclusion in the next item we need that, when $i$ appears simultaneously in both runs, the mapping $\Phi(s) = s + \Delta(s)$ is nondecreasing:
\begin{equation}
s_1 \le s_2 \;\Rightarrow\; \Phi(s_1) \le \Phi(s_2).
\end{equation}
For $\Delta(s) = c/s$ we have
\begin{equation}
\Phi(s) \,=\, s + \frac{c}{s}, \qquad \Phi'(s) \,=\, 1 - \frac{c}{s^2} \;\ge\; 0 \quad \text{for } s \ge \sqrt{c}.
\end{equation}
Here is why we quickly enter the region $s \ge \sqrt{c}$ : 

The arithmetic mean of nonnegative numbers is at least the geometric mean:
\begin{equation}
s + \frac{c}{s} \;\ge\; 2\sqrt{c} \quad (s>0).
\end{equation}
Hence, after any single occurrence:
\begin{equation}
s_{\mathrm{new}} \,=\, \Phi(s) \,=\, s + \frac{c}{s} \;\ge\; 2\sqrt{c} \;\ge\; \sqrt{c} ,
\end{equation}
thus $s_{\mathrm{new}} \ge \sqrt{c}$. On the next step, if $c$ does not change much, the monotonicity of $\Phi$ persists: $\Phi'(s) \ge 0$. Hence $\Phi$ cannot make the "large" vector smaller than the "small" one under the same event ("item $i$ appeared at least once in the collected batch"). The fact that $c$ does not grow too fast at each step is proved in attachment Z.

\paragraph{4) Induction over steps: why larger $p_i$ implies no smaller outcome.}
Synchronize randomness as in item~1 and iterate over batches $t$:
\begin{itemize}
  \item if $i$ is absent in both runs, both states stay unchanged;
  \item if $i$ appears only in the $p_i'$-run, its state increases there while the $p_i$-run stays put;
  \item if $i$ appears in both runs, the nondecreasing $\Phi$ preserves the order (Look for Y attachment for extra comments on "Note on batch difference").
\end{itemize}
By induction on $t$ (almost surely on the coupled randomness), we obtain $s^{(T)}_{p_i} \le s^{(T)}_{p_i'}$. Taking expectations yields the desired monotonicity:
\begin{equation}
\boxed{\; \mathbb E\,\bigl\|q_i^{(T)}\bigr\| \text{ is nondecreasing in } p_i \; }.
\end{equation}
Look for Y attachment for extra comments on "Note on batch difference".
