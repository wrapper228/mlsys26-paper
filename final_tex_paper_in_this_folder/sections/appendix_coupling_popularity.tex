\section{Popularity Dependence via Coupling}
\label{app:popularity-dependence}

We aim to show that, after $T$ training batches, the embedding norm of item $i$ is nondecreasing in its sampling probability $p_i$:
\begin{equation}
\mathbb E\,\bigl\|q_i^{(T)}\bigr\| \;\text{ is nondecreasing in }\; p_i.
\end{equation}
We employ the coupling method from probability theory (see "Modern Discrete Probability: An Essential Toolkit").

\paragraph{1) Two runs on the same randomness (we fixed the random outcome of batch generation).}
Run training twice, differing only in the sampling probability of item $i$: $p_i' < p_i''$. At each step $t$ and for every slot of the batch draw the same random number $U \sim \mathrm{Uniform}(0,1)$:
\begin{itemize}
  \item in the run with parameter $p$, place $i$ into the slot if $U \le p_i$;
  \item otherwise place the same non-$i$ item in both runs (chosen using the same random generator).
\end{itemize}
Then, for the batch collected at step $t$,
\begin{equation}
\begin{aligned}
&\mathbf{Indicator}\{\text{item } i \text{ appears at least once under } p_i''\} \\
&\;\ge\; \\
&\mathbf{Indicator}\{\text{item } i \text{ appears at least once under } p_i'\}.
\end{aligned}
\end{equation}
Thus, with the larger $p_i$ the number of appearances of $i$ is never smaller.

Look for X attachment for extra comments on "Note on distributions".

\paragraph{2) What a single occurrence does to the norm.}
Let $s^{(t)} = \bigl\|q_i^{(t)}\bigr\|^2$. A one-step update satisfies
\begin{equation}
s^{(t+1)} = \begin{cases}
s^{(t)}, & \text{if item $i$ does not occur in batch $t$,} \\
s^{(t)} + \Delta\bigl(s^{(t)}\bigr), & \text{if item $i$ occurs at least once.}
\end{cases}
\end{equation}
Under orthogonal updates, the Pythagorean formula gives
\begin{equation}
\label{eq:pythagoras-increment}
\bigl\|q + \Delta q\bigr\|^2 \;=\; \|q\|^2 + \bigl\|\Delta q\bigr\|^2, \qquad \Rightarrow\; \Delta\bigl(s^{(t)}\bigr) = \bigl\|\Delta q_i^{(t)}\bigr\|^2 \ge 0.
\end{equation}
For cosine-based losses (the gradient with respect to $q$ is orthogonal to $q$), a typical dependence is
\begin{equation}
\Delta(s) \,=\, \frac{c}{s}, \qquad c>0, \qquad \text{see Appendix~\ref{app:cosine-growth-rate}}.
\end{equation}
Thus: the larger the norm, the smaller the increment, yet the increment remains strictly positive.

\paragraph{3) Requirement: the step must not reverse the order.}
For the final conclusion we need that, when $i$ occurs simultaneously in both runs, the mapping $\Phi(s) = s + \Delta(s)$ is nondecreasing:
\begin{equation}
s_1 \le s_2 \;\Rightarrow\; \Phi(s_1) \le \Phi(s_2).
\end{equation}
For $\Delta(s) = c/s$ we have
\begin{equation}
\Phi'(s) \,=\, 1 - \frac{c}{s^2} \;\ge\; 0 \quad \text{whenever } s \ge \sqrt{c}.
\end{equation}
Why do we quickly enter the region $s \ge \sqrt{c}$? By the arithmeticâ€“geometric mean inequality (the arithmetic mean is at least the geometric mean),
\begin{equation}
s + \frac{c}{s} \;\ge\; 2\sqrt{c} \quad (c>0).
\end{equation}
Hence, after any single occurrence,
\begin{equation}
s_{\mathrm{new}} \,=\, s + \frac{c}{s} \;\ge\; 2\sqrt{c} \;\ge\; \sqrt{c},
\end{equation}
and from then on $\Phi$ is nondecreasing at every joint occurrence. That $c$ does not grow too fast at every step is proved in Z attachment.

\paragraph{4) Induction over steps: why larger $p_i$ implies no smaller outcome.}
Synchronize randomness as in item~1 and iterate over batches $t$:
\begin{itemize}
  \item if $i$ is absent in both runs, both states stay unchanged;
  \item if $i$ appears only in the $p_i'$-run, its state increases there while the $p_i$-run stays put;
  \item if $i$ appears in both runs, the nondecreasing $\Phi$ preserves the order (Look for Y attachment for extra comments on "Note on batch difference").
\end{itemize}
By induction on $t$ (almost surely on the coupled randomness), we obtain $s^{(T)}_{p_i} \le s^{(T)}_{p_i'}$. Taking expectations yields the desired monotonicity:
\begin{equation}
\boxed{\; \mathbb E\,\bigl\|q_i^{(T)}\bigr\| \text{ is nondecreasing in } p_i \; }.
\end{equation}
Look for Y attachment for extra comments on "Note on batch difference".
