\section{Experiments: Empirical Validation of Theoretical Results}
\label{sec:experiments}

\begin{table*}[!t]
\centering
\small
\begin{tabular}{@{}cccccccc@{}}
\toprule
No. & \begin{tabular}{@{}c@{}} Left (user) \\ encoder \end{tabular} & Loss & \begin{tabular}{@{}c@{}} Cond.~1 for \\ left encoder? \end{tabular} & Cond.~2? & \begin{tabular}{@{}c@{}} Orthogonal \\ trajectory? \end{tabular} & \begin{tabular}{@{}c@{}} Sufficient orthogonal \\ displacement? \end{tabular} & \begin{tabular}{@{}c@{}} Popularity \\ bias? \end{tabular} \\
\midrule
8 & Embedding & InfoNCE (cos) & $\checkmark$ & $\checkmark$ & $\checkmark$ & \begin{tabular}{@{}c@{}} $\checkmark$ \\ (high learning rate) \end{tabular} & $\checkmark$ \\
9 & Embedding & InfoNCE (cos) & $\checkmark$ & $\checkmark$ & $\checkmark$ & \begin{tabular}{@{}c@{}} $\times$ \\ (low learning rate) \end{tabular} & $\times$ \\
\bottomrule
\end{tabular}
\caption{Do orthogonal trajectories produce popularity bias? Condition~1: single parameter-linear layer over one-hot inputs or a single embedding layer; Condition~2: loss gradient orthogonal to the encoder output. The last column indicates whether the orthogonal displacement per step is sufficiently large.}\label{tab:orthogonality-popbias}
\end{table*}

To clarify scope, we do not assess recommendation quality on held-out datasets and we do not report relevance metrics (e.g., MAP@k). 
Instead, we empirically examine geometric properties of the training dynamics across encoder architectures and training designs, focusing on the orthogonality of embedding updates and the frequencyâ€“norm coupling predicted by the theory.
All datasets and implementation details are available in a public GitHub repository X (link to be added).

\input{sections/experiments_orthogonality_factors}
\input{sections/experiments_orthogonality_popularity_bias}
\input{sections/experiments_practical_two_tower_bias}
