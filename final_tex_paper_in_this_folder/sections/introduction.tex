% Introduction
\section{Introduction}
Two-tower architectures are the de facto standard for large-scale retrieval and recommendation. Their efficiency stems from decoupling the user and item encoders and scoring by a simple similarity, which enables precomputation, approximate nearest-neighbor search, and low-latency serving at web-scale. Yet the role of embedding magnitudes in training and ranking remains under-characterized: empirical observations vary across systems, and theory has not reached consensus on when and why norms change under different objectives and optimizers.

From the definition of cosine similarity, \(\cos(q,k)=\langle q,k\rangle/(\|q\|\,\|k\|)\), the normalization appears to cancel the effect of magnitudes, so objectives that couple the left and right encoder embeddings only through cosine similarity are expected to be insensitive to embedding norms. A competing line of analysis argues that for cosine-based losses the gradient \(\nabla_{q}\,\mathcal{L}\) is orthogonal to \(q\), and therefore any gradient step must increase \(\|q\|\), often extrapolated to deep encoders. Both perspectives ignore how parameter updates propagate through the encoder and fail to state the algorithmic and architectural conditions under which norm growth necessarily occurs--or fails to occur.

I replace black-box reasoning with an update-level analysis. I track how parameter updates map to item-embedding displacements and prove that, under explicit and testable premises—the item encoder is linear in its parameters with non-shared per-item parameters; the item side is trained by plain SGD; and the loss uses the item embedding only via cosine similarities—each step moves the embedding orthogonally to its current value and thus monotonically increases its norm. From this, I derive a frequency--norm law: expected radial growth increases with item sampling frequency and diminishes with the current norm, yielding a concrete mechanism for popularity bias in asymmetric two-tower systems.

The analysis also delineates the limits of claims that ``gradient orthogonality implies norm inflation'' for deep encoders. Orthogonality of the output gradient is insufficient on its own; orthogonality of the \emph{update} follows only when the encoder's Jacobian preserves it, which holds precisely under the stated premises. Violating any premise--using Adam on the item table, adding nonlinearities or parameter sharing in the item tower, or optimizing dot product--breaks the guarantee and leads to non-systematic norm dynamics.

\paragraph{Contributions.}
\begin{itemize}
    \item \textbf{Mechanism and conditions.} A first-order, update-level characterization that identifies when item-embedding steps are orthogonal and norms grow monotonically in cosine-trained two-tower models with a parameter-linear item encoder optimized by SGD.
    \item \textbf{Frequency–norm law.} A derivation linking item sampling frequency to expected radial growth with a diminishing-returns term, explaining the emergence and scaling of popularity bias.
    \item \textbf{Clarification of prior claims.} A precise account of why output-gradient orthogonality alone does not imply norm inflation for deep encoders, together with the exact premises under which it does.
    \item \textbf{Geometric instrumentation and evidence.} A diagnostic that decomposes updates into tangential and radial components and empirical results showing orthogonal motion and a robust, statistically significant frequency–norm coupling when the premises hold, and its disappearance when any premise is flipped.
\end{itemize}

\paragraph{Scope and implications.} TODO

\paragraph{Paper organization.} TODO