% Introduction
\section{Introduction}
Two-tower architectures are the de-facto standard for large-scale retrieval and recommendation. Their efficiency stems from decoupling the user and item encoders and scoring by a simple similarity, which enables precomputation, approximate nearest-neighbor search, and low-latency serving at web scale. Despite these advantages, production systems report a persistent popularity bias: more frequently sampled items acquire larger embedding norms and thereby receive inflated dot-product scores at ranking time, even when training uses cosine-based objectives intended to be insensitive to magnitude. This creates feedback loops that over-expose head items, distorts calibration, and degrades long-tail utility.

Existing explanations are unsatisfactory. On one side, cosine objectives are often informally argued to be “norm-agnostic,” suggesting they should suppress magnitude effects. On the other, previous studies have claimed that the gradient is orthogonal to an embedding and therefore any gradient step must increase its norm, implicitly extending this claim to deep encoders. Both views treat the encoder as a black box and blur the distinction between gradients with respect to outputs and updates applied through parameters. As a result, they neither identify the concrete algorithmic and architectural preconditions under which norm growth must occur nor delineate regimes where it should not.

This paper resolves that gap by replacing black-box reasoning with an explicit update-level analysis. I study how encoder parameters drive changes of item embeddings across training steps and show that, under identifiable premises, the per-step embedding displacement is orthogonal to the current embedding and therefore strictly increases its norm. Crucially, the premises are operational and testable in real systems: the item encoder is linear in its parameters with non-shared per-item parameters (e.g., an embedding table or an equivalent linear map over one-hot inputs); training of the item side uses plain SGD; and the loss depends on an item only through cosine similarities within an InfoNCE-style objective. Under these conditions, I derive an explicit dependence of expected radial (norm) growth on item sampling frequency with a countervailing dependence on the current norm, yielding a clear mechanism for the observed frequency–norm coupling.

The analysis also clarifies the limits of prior claims that “gradient orthogonality implies norm inflation” for deep encoders. Orthogonality of the gradient with respect to the embedding output is not sufficient: what matters is the composition of parameter updates through the encoder’s Jacobian at the item, which preserves orthogonality only under the premises above. When these premises are violated—by using Adam on the item table, introducing nonlinearities or parameter sharing in the item tower, or optimizing dot-product instead of cosine—the radial effect no longer follows, and norm dynamics become non-systematic. Overall, my key contributions include:

\begin{itemize}
  \item \textbf{Mechanism and conditions.} I provide a first-order, update-level characterization of when item-embedding steps are orthogonal to the current embedding, implying monotone norm growth. The result applies to asymmetric two-tower systems with a parameter-linear item encoder trained by SGD under cosine-based losses.
  \item \textbf{Frequency–norm law.} I derive a link between item sampling frequency and expected radial growth, explaining why popularity bias emerges and how it scales with exposure, along with a diminishing-returns term that slows growth for already large norms.
  \item \textbf{Clarification of prior claims.} I show why gradient orthogonality at the output does not generally entail norm inflation for deep encoders and specify the precise premises under which it does.
  \item \textbf{Geometry-first instrumentation.} I introduce a simple diagnostic that decomposes each item update into tangential and radial components, enabling direct measurement of orthogonality and norm growth in training logs.
  \item \textbf{Empirical support.} Across controlled two-tower setups that satisfy the premises, I observe strictly orthogonal item-side displacements and a robust, **statistically significant** correlation between item frequency and learned norms; toggling any single premise collapses radial increments to noise and removes the frequency–norm coupling.
\end{itemize}

\paragraph{Scope and implications.} TODO

\paragraph{Paper organization.} TODO