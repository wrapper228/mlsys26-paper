\section{Embedding-Norm Is Monotonic in Item Popularity}
\label{sec:norm-monotonicity}

Throughout this section (as well as corresponding appendices) we assume the four conditions from Section~2 hold (SGD without momentum/regularization; parameter-linear encoder; non-shared parameter rows; cosine-loss gradient orthogonal to the output). Under these assumptions the embedding moves orthogonally at each update. By the Pythagorean theorem,
\begin{equation}
\label{eq:pythagoras-increment}
\bigl\|q_i^{(t+1)}\bigr\|^{2} - \bigl\|q_i^{(t)}\bigr\|^{2} \;=\; \bigl\|\Delta q_i^{(t)}\bigr\|^{2}.
\end{equation}

Before analyzing the dependence on popularity, we state a claim and provide its proof in the appendix:
\begin{quote}
\textbf{Claim.} The larger the embedding norm, the slower it grows under a cosine-based loss.
\end{quote}
The proof (Appendix~\ref{app:cosine-growth-rate}) derives an explicit expression for the norm of the gradient and shows its inverse dependence on the current norm of $q$.
We use this claim in the following Appendix~\ref{app:popularity-dependence}.

Finally, we analyze the dependence of the embedding-norm growth on item popularity. Using a coupling argument (Appendix~\ref{app:popularity-dependence}), we show that the expected \emph{squared} embedding norm after $T$ batches is nondecreasing in the item's sampling probability.

\paragraph{Takeaway.} This section and the corresponding appendices provide a formal mechanism for popularity bias: under the four assumptions of Section~2, more frequently sampled items accumulate larger squared norms (and hence larger norms). 
To observe a statistically significant popularity bias in practice, an additional (fifth) factor is required; it will be discussed in the next section.
