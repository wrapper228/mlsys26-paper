\section{Note on variability of $c$}
\label{app:note-on-c-variability}

Recall the notation from Appendix~\ref{app:cosine-growth-rate}. Let
\begin{equation}
s \,=\, \|q\|^{2}, \qquad P \,=\, I - \hat q\,\hat q^{\!\top}, \qquad
u \,=\, \sum_{i=1}^{m} \frac{\partial F}{\partial c_i}\,\hat k_i .
\end{equation}
With a learning-rate factor $\eta$, a single-update norm increment satisfies
\begin{equation}
\begin{aligned}
\Delta(s) &= \eta^{2}\,\bigl\|\nabla_q L(q)\bigr\|^{2}
          = \frac{\eta^{2}}{s}\,\|Pu\|^{2}
          = \frac{c}{s}, \\
c &= \eta^{2}\,\|Pu\|^{2}.
\end{aligned}
\end{equation}

\paragraph{Why $c$ changes moderately (or sharply, but rarely).\\\\}

\textbf{1) $\|u_{t+1}\| \gg \|u_t\|$ is a rare event:}

In a convergent training process the embeddings move to their optimal positions and the ``error signals'' $\tfrac{\partial F}{\partial cos}$, if they do not decay, at least stabilize. Therefore the norm of the aggregate gradient
\begin{equation}
u \,=\, \sum_{i=1}^{m} \frac{\partial F}{\partial cos_i}\,\hat k_i
\end{equation}
cannot keep growing sharply for many steps; a persistent sharp growth of the norm of the aggregate gradient $u$ would signal a diverging training process.

At a heuristic level, a sharp growth of the norm of $u$ would require that, simultaneously, all contributions $\hat k_i$ add up co-directionally in the tangent plane to $q$ â€” an event I consider unlikely under random batch formation. Even if for some reason this happens often, the sum of contributions is limited by the number of pairs with this item in the training set and by the value of $\tfrac{\partial F}{\partial cos}$, whose stability I rigorously justified above.

\textbf{2) If this rare spike happens and temporarily yields $s < \sqrt{c}$ :}

On the same step we have
\begin{equation}
s_{t+1} \,=\, s_t + \frac{c_t}{s_t} \;\ge\; 2\sqrt{c_t} \;\ge\; \sqrt{c_t} ,
\end{equation}
so we immediately re-enter the region where $\Phi'(s) \ge 0$ again. With an increased $s$, violating $\Phi'(s) \ge 0$ again becomes progressively harder.
