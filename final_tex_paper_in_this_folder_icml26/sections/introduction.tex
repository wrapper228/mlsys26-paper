% Introduction
\section{Introduction}
Two-tower architectures are the de facto standard for large-scale retrieval and recommendation \cite{huang2024retrievalsurvey}. Their efficiency stems from decoupling the user and item encoders and scoring by a simple similarity, which enables precomputation, approximate nearest-neighbor search, and low-latency serving at web-scale. Yet the role of embedding magnitudes in training and ranking remains under-characterized: empirical observations vary across systems, and theory has not reached consensus on when and why norms change under different objectives and optimizers.

From the definition of cosine similarity, \(\cos(q,k)=\langle q,k\rangle/(\|q\|\,\|k\|)\), the normalization appears to cancel the effect of magnitudes, so objectives that couple the left and right encoder embeddings only through cosine similarity are expected to be insensitive to embedding norms. A competing line of analysis argues \cite{wang2017normface,draganov2024pitfalls,draganov2025embeddingnorms} that for cosine-based losses the gradient \(\nabla_{q}\,\mathcal{L}\) is orthogonal to \(q\), and therefore any gradient step must increase \(\|q\|\), often extrapolated to deep encoders. Both perspectives ignore how parameter updates propagate through the encoder and do not explicitly state the algorithmic and architectural conditions under which norm growth necessarily occurs.

I replace black-box reasoning with an update-level analysis. I track how parameter updates map to item-embedding displacements and prove that, under explicit and testable premises — the item encoder is linear in its parameters with non-shared per-item parameters; the item side is trained by plain SGD; and the loss uses the item embedding only via cosine similarities — each step moves the embedding orthogonally to its current value and thus monotonically increases its norm. From this, I derive a frequency–norm law: expected radial growth increases with item sampling frequency and diminishes with the current norm, yielding a concrete mechanism for popularity bias in two-tower systems.

The analysis also delineates the limits of claims that ``gradient orthogonality implies norm inflation'' for deep encoders \cite{wang2017normface,draganov2024pitfalls,draganov2025embeddingnorms}. Orthogonality of the output gradient is insufficient on its own; orthogonality of the \emph{update} follows only when the encoder's Jacobian preserves it, which holds precisely under the stated premises. Violating any premise — adding nonlinearities or parameter sharing in the item tower or optimizing dot product — breaks the guarantee and leads to non-systematic norm dynamics. My contributions are:

\begin{itemize}
    \item \textbf{Explanation of sufficient conditions for orthogonality of embedding updates.} I prove that embeddings move orthogonally under the following premises: (A1) the item encoder is linear in its parameters; (A2) per-item parameters are not shared; (A3) the loss depends on the item only via cosine; and (A4) the item side is optimized by SGD without regularization and momentum. Orthogonality breaks if any premise is violated.
    \item \textbf{Explanation of why orthogonality of embedding updates implies norm growth.} Using \emph{coupling} probabilistic technique I prove that orthogonal movement makes more popular item have systematically bigger embedding norm.  
    \item \textbf{Clarification of prior claims in the literature.} I re-examine claims from studies on similar topic \cite{wang2017normface,draganov2024pitfalls,draganov2025embeddingnorms} and argue that gradient update affects the model parameters, not its output itself.
    \item \textbf{Common two-tower configurations that produce popularity bias.} Given complexity of modern architectures, the guarantees on popularity bias can still hold in production-style two-tower setups if item tower meet premises.
\end{itemize}

All my claims are supported by experimental results.

\paragraph{Scope and implications.} I establish sufficient conditions under which embedding updates are orthogonal and popularity bias emerges. Outside this regime, these effects may or may not occur; I make no claim of necessity. Other sources of popularity bias are examined in prior work \cite{zhang2023dimensional}; I focus on frequency–norm coupling as a distinct mechanism.